{
  "nbformat": 4,
  "nbformat_minor": 5,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.10"
    },
    "colab": {
      "name": "Spark and Python for Big Data.ipynb",
      "provenance": [],
      "collapsed_sections": []
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qc7kCc2NSBBq"
      },
      "source": [
        "# NLP Using PySpark"
      ],
      "id": "Qc7kCc2NSBBq"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gBr8D3FtSBBr"
      },
      "source": [
        "## Objective:\n",
        "- The objective from this project is to create a <b>Spam filter using NaiveBayes classifier</b>.\n",
        "- It is required to obtain <b>f1_scored > 0.9</b>.\n",
        "- We'll use a dataset from UCI Repository. SMS Spam Detection: https://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection\n",
        "- Data is also provided for you in the assignment (you do not have to download it)."
      ],
      "id": "gBr8D3FtSBBr"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SWbOfaCfSBBt"
      },
      "source": [
        "### Create a spark session and import the required libraries"
      ],
      "id": "SWbOfaCfSBBt"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EFFGV2ndSBBv"
      },
      "source": [
        "import findspark\n",
        "findspark.init()\n",
        "import pyspark\n",
        "from pyspark.sql import SparkSession\n",
        "spark = SparkSession.builder.appName(\"Project\").getOrCreate()"
      ],
      "id": "EFFGV2ndSBBv",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yt2f8syzSBBy"
      },
      "source": [
        "### Read the readme file to learn more about the data"
      ],
      "id": "yt2f8syzSBBy"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kWMFJclNSBB0"
      },
      "source": [
        "### Read the data into a DataFrame"
      ],
      "id": "kWMFJclNSBB0"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cqg0e-WLSBB1"
      },
      "source": [
        "data = spark.read.csv(\"SMSSpamCollection\", inferSchema=\"true\", sep='\\t')"
      ],
      "id": "cqg0e-WLSBB1",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZkSZIozSSBB2"
      },
      "source": [
        "### Print the schema"
      ],
      "id": "ZkSZIozSSBB2"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EzUNynPWSBB3",
        "outputId": "dc13b21d-a09a-4d25-ceff-45b99c365c2c"
      },
      "source": [
        "data.printSchema()"
      ],
      "id": "EzUNynPWSBB3",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- _c0: string (nullable = true)\n",
            " |-- _c1: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "21YOYRO4SBB5"
      },
      "source": [
        "### Rename the first column to 'class' and second column to 'text'"
      ],
      "id": "21YOYRO4SBB5"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l-kYpDbZSBB6",
        "outputId": "77c92214-f9d8-4bf0-8685-ed9a726cf817"
      },
      "source": [
        "data = data.selectExpr(\"_c0 as class\", \"_c1 as text\")\n",
        "data.printSchema()"
      ],
      "id": "l-kYpDbZSBB6",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- class: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ljc7reZNSBB7"
      },
      "source": [
        "### Show the first 10 rows from the dataframe"
      ],
      "id": "ljc7reZNSBB7"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z0uYgsDjSBB7",
        "outputId": "4f57c607-bda7-4110-f4b1-58084c7d0700"
      },
      "source": [
        "data.show(10)"
      ],
      "id": "z0uYgsDjSBB7",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+\n",
            "|class|                text|\n",
            "+-----+--------------------+\n",
            "|  ham|Go until jurong p...|\n",
            "|  ham|Ok lar... Joking ...|\n",
            "| spam|Free entry in 2 a...|\n",
            "|  ham|U dun say so earl...|\n",
            "|  ham|Nah I don't think...|\n",
            "| spam|FreeMsg Hey there...|\n",
            "|  ham|Even my brother i...|\n",
            "|  ham|As per your reque...|\n",
            "| spam|WINNER!! As a val...|\n",
            "| spam|Had your mobile 1...|\n",
            "+-----+--------------------+\n",
            "only showing top 10 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCMcp40pSBB8"
      },
      "source": [
        "## Clean and Prepare the Data"
      ],
      "id": "MCMcp40pSBB8"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-qFNdeRMSBB9"
      },
      "source": [
        "### Create a new feature column contains the length of the text column"
      ],
      "id": "-qFNdeRMSBB9"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XOjOtaEJSBB9"
      },
      "source": [
        "from pyspark.sql.functions import length\n",
        "data = data.withColumn('length',length(data['text']))"
      ],
      "id": "XOjOtaEJSBB9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IS7whvdSSBB-"
      },
      "source": [
        "### Show the new dataframe"
      ],
      "id": "IS7whvdSSBB-"
    },
    {
      "cell_type": "code",
      "metadata": {
        "scrolled": false,
        "id": "M-eNLJ5mSBB_",
        "outputId": "9afd9427-df08-4a8a-f1c0-1bd72961f1e4"
      },
      "source": [
        "data.show()"
      ],
      "id": "M-eNLJ5mSBB_",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+------+\n",
            "|class|                text|length|\n",
            "+-----+--------------------+------+\n",
            "|  ham|Go until jurong p...|   111|\n",
            "|  ham|Ok lar... Joking ...|    29|\n",
            "| spam|Free entry in 2 a...|   155|\n",
            "|  ham|U dun say so earl...|    49|\n",
            "|  ham|Nah I don't think...|    61|\n",
            "| spam|FreeMsg Hey there...|   147|\n",
            "|  ham|Even my brother i...|    77|\n",
            "|  ham|As per your reque...|   160|\n",
            "| spam|WINNER!! As a val...|   157|\n",
            "| spam|Had your mobile 1...|   154|\n",
            "|  ham|I'm gonna be home...|   109|\n",
            "| spam|SIX chances to wi...|   136|\n",
            "| spam|URGENT! You have ...|   155|\n",
            "|  ham|I've been searchi...|   196|\n",
            "|  ham|I HAVE A DATE ON ...|    35|\n",
            "| spam|XXXMobileMovieClu...|   149|\n",
            "|  ham|Oh k...i'm watchi...|    26|\n",
            "|  ham|Eh u remember how...|    81|\n",
            "|  ham|Fine if thatÂ’s th...|    56|\n",
            "| spam|England v Macedon...|   155|\n",
            "+-----+--------------------+------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hu9Ra-4WSBB_"
      },
      "source": [
        "### Get the average text length for each class "
      ],
      "id": "hu9Ra-4WSBB_"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N7BtmfJ7SBCA",
        "outputId": "8c2e9277-bf94-4a10-8009-acb069511aa1"
      },
      "source": [
        "from pyspark.sql.functions import mean\n",
        "data.groupby('class').agg(mean('length').alias('length average')).show()"
      ],
      "id": "N7BtmfJ7SBCA",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+-----------------+\n",
            "|class|   length average|\n",
            "+-----+-----------------+\n",
            "|  ham|71.45431945307645|\n",
            "| spam|138.6706827309237|\n",
            "+-----+-----------------+\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GtSTM0feSBCB"
      },
      "source": [
        "## Feature Transformations"
      ],
      "id": "GtSTM0feSBCB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkh7vRcdSBCB"
      },
      "source": [
        "### In this part you transform you raw text in to tf_idf model :"
      ],
      "id": "hkh7vRcdSBCB"
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OY4Pjz_7SBCC"
      },
      "source": [
        "### Perform the following steps to obtain TF-IDF:\n",
        "1. Import the required transformers/estimators for the subsequent steps.\n",
        "2. Create a <b>Tokenizer</b> from the text column.\n",
        "3. Create a <b>StopWordsRemover</b> to remove the <b>stop words</b> from the column obtained from the <b>Tokenizer</b>.\n",
        "4. Create a <b>CountVectorizer</b> after removing the <b>stop words</b>.\n",
        "5. Create the <b>TF-IDF</b> from the <b>CountVectorizer</b>."
      ],
      "id": "OY4Pjz_7SBCC"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OS7nQQ_lSBCC"
      },
      "source": [
        "from pyspark.ml.feature import Tokenizer, \\\n",
        "                               StopWordsRemover, \\\n",
        "                               CountVectorizer,\\\n",
        "                               IDF,StringIndexer, \\\n",
        "                               VectorAssembler"
      ],
      "id": "OS7nQQ_lSBCC",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dEVU2UDwSBCD"
      },
      "source": [
        "tokenizer = Tokenizer(inputCol=\"text\", outputCol=\"tokenizer_text\")"
      ],
      "id": "dEVU2UDwSBCD",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dB2qcnoiSBCE"
      },
      "source": [
        "stopWordsRemove = StopWordsRemover(inputCol='tokenizer_text',outputCol='text_without_stopWords')"
      ],
      "id": "dB2qcnoiSBCE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Fm98wDEHSBCE"
      },
      "source": [
        "count_vector = CountVectorizer(inputCol='text_without_stopWords',outputCol='count_vec')"
      ],
      "id": "Fm98wDEHSBCE",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HEVW9jUqSBCF"
      },
      "source": [
        "- Convert the <b>class column</b> to index using <b>StringIndexer</b>\n",
        "- Create feature column from the <b>TF-IDF</b> and <b>lenght</b> columns."
      ],
      "id": "HEVW9jUqSBCF"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZiItdsonSBCF"
      },
      "source": [
        "indexer = StringIndexer(inputCol='class', outputCol='label')"
      ],
      "id": "ZiItdsonSBCF",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-K7ZzNvgSBCG"
      },
      "source": [
        "idf = IDF(inputCol=\"count_vec\", outputCol=\"tf_idf\")"
      ],
      "id": "-K7ZzNvgSBCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7bYfUyicSBCG"
      },
      "source": [
        "vectorAssembler = VectorAssembler(inputCols=['tf_idf','length'], outputCol='features')"
      ],
      "id": "7bYfUyicSBCG",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n7sWv36WSBCH"
      },
      "source": [
        "## The Model\n",
        "- Create a <b>NaiveBayes</b> classifier with the default parameters."
      ],
      "id": "n7sWv36WSBCH"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SUQSVc6FSBCH"
      },
      "source": [
        "from pyspark.ml.classification import NaiveBayes"
      ],
      "id": "SUQSVc6FSBCH",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xo1vb2DBSBCI"
      },
      "source": [
        "nb = NaiveBayes()"
      ],
      "id": "Xo1vb2DBSBCI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1Tbc-fhDSBCI"
      },
      "source": [
        "## Pipeline\n",
        "### Create a pipeline model contains all the steps starting from the Tokenizer to the NaiveBays classifier."
      ],
      "id": "1Tbc-fhDSBCI"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6Th9WyVRSBCI"
      },
      "source": [
        "from pyspark.ml import Pipeline"
      ],
      "id": "6Th9WyVRSBCI",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Y6TJ-vwSBCJ"
      },
      "source": [
        "pipeline = Pipeline(stages=[tokenizer, indexer, stopWordsRemove, count_vector, idf, vectorAssembler, nb])"
      ],
      "id": "8Y6TJ-vwSBCJ",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lgbjQDCtSBCJ"
      },
      "source": [
        "### Split your data to trian and test data with ratios 0.7 and 0.3 respectively."
      ],
      "id": "lgbjQDCtSBCJ"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "537lLUGxSBCK"
      },
      "source": [
        "(training,testing) = data.randomSplit([0.7, 0.3])"
      ],
      "id": "537lLUGxSBCK",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5_qkVOAJSBCK"
      },
      "source": [
        "### Fit your Pipeline model to the training data"
      ],
      "id": "5_qkVOAJSBCK"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UJkYJfylSBCL"
      },
      "source": [
        "model = pipeline.fit(training)"
      ],
      "id": "UJkYJfylSBCL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sRTfDjd5SBCL"
      },
      "source": [
        "### Perform predictions on tests dataframe"
      ],
      "id": "sRTfDjd5SBCL"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ox3O42h7SBCM",
        "outputId": "a750c7e9-1ae6-4d5e-ea76-eb0978c6ab71"
      },
      "source": [
        "prediction = model.transform(testing)\n",
        "prediction.show()"
      ],
      "id": "Ox3O42h7SBCM",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "+-----+--------------------+------+--------------------+-----+----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|class|                text|length|      tokenizer_text|label|text_without_stopWords|           count_vec|              tf_idf|            features|       rawPrediction|         probability|prediction|\n",
            "+-----+--------------------+------+--------------------+-----+----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "|  ham| &lt;DECIMAL&gt; ...|   132|[, &lt;decimal&gt...|  0.0|  [, &lt;decimal&gt...|(10824,[3,71,107,...|(10824,[3,71,107,...|(10825,[3,71,107,...|[-691.47588728510...|[1.0,1.9593202765...|       0.0|\n",
            "|  ham| and  picking the...|    41|[, and, , picking...|  0.0|  [, , picking, var...|(10824,[3,1336],[...|(10824,[3,1336],[...|(10825,[3,1336,10...|[-120.98704346268...|[1.0,8.8681084061...|       0.0|\n",
            "|  ham| said kiss, kiss,...|   133|[, said, kiss,, k...|  0.0|  [, said, kiss,, k...|(10824,[3,94,250,...|(10824,[3,94,250,...|(10825,[3,94,250,...|[-655.23394362864...|[1.0,5.9994156623...|       0.0|\n",
            "|  ham| what number do u...|    36|[, what, number, ...|  0.0|  [, number, u, liv...|(10824,[0,3,81,17...|(10824,[0,3,81,17...|(10825,[0,3,81,17...|[-204.08329059615...|[0.99999999981705...|       0.0|\n",
            "|  ham|\"Happy valentines...|   147|[\"happy, valentin...|  0.0|  [\"happy, valentin...|(10824,[15,164,19...|(10824,[15,164,19...|(10825,[15,164,19...|[-503.74341396277...|[1.0,5.6627538910...|       0.0|\n",
            "|  ham|\"Speak only when ...|    80|[\"speak, only, wh...|  0.0|  [\"speak, feel, wo...|(10824,[117,120,1...|(10824,[117,120,1...|(10825,[117,120,1...|[-206.29748338289...|[1.0,1.8278552218...|       0.0|\n",
            "|  ham|\"The world suffer...|   129|[\"the, world, suf...|  0.0|  [\"the, world, suf...|(10824,[17,120,24...|(10824,[17,120,24...|(10825,[17,120,24...|[-449.87543383473...|[1.0,7.8972074138...|       0.0|\n",
            "|  ham|&lt;#&gt; %of ppl...|   327|[&lt;#&gt;, %of, ...|  0.0|  [&lt;#&gt;, %of, ...|(10824,[0,2,3,5,6...|(10824,[0,2,3,5,6...|(10825,[0,2,3,5,6...|[-1564.0199124681...|[1.0,9.9468547216...|       0.0|\n",
            "|  ham|(And my man carlo...|    66|[(and, my, man, c...|  0.0|  [(and, man, carlo...|(10824,[149,425,6...|(10824,[149,425,6...|(10825,[149,425,6...|[-365.60868614494...|[1.0,9.5296768869...|       0.0|\n",
            "|  ham|(I should add tha...|   132|[(i, should, add,...|  0.0|  [(i, add, really,...|(10824,[5,18,60,1...|(10824,[5,18,60,1...|(10825,[5,18,60,1...|[-505.75495048296...|[1.0,1.2994907006...|       0.0|\n",
            "|  ham|(That said can yo...|    43|[(that, said, can...|  0.0|  [(that, said, tex...|(10824,[22,24,94]...|(10824,[22,24,94]...|(10825,[22,24,94,...|[-111.39679889400...|[0.99999999995353...|       0.0|\n",
            "|  ham|* Was really good...|    69|[*, was, really, ...|  0.0|  [*, really, good,...|(10824,[17,30,46,...|(10824,[17,30,46,...|(10825,[17,30,46,...|[-281.13574456562...|[1.0,1.2958441449...|       0.0|\n",
            "|  ham|*deep sigh* ... I...|   129|[*deep, sigh*, .....|  0.0|  [*deep, sigh*, .....|(10824,[5,32,60,7...|(10824,[5,32,60,7...|(10825,[5,32,60,7...|[-769.23372801281...|[1.0,3.7801330838...|       0.0|\n",
            "|  ham|, ,  and  picking...|   169|[,, ,, , and, , p...|  0.0|  [,, ,, , , pickin...|(10824,[0,2,3,8,2...|(10824,[0,2,3,8,2...|(10825,[0,2,3,8,2...|[-1046.8755982753...|[1.0,8.1365372115...|       0.0|\n",
            "|  ham|, how's things? J...|    38|[,, how's, things...|  0.0|  [,, things?, quic...|(10824,[242,2556,...|(10824,[242,2556,...|(10825,[242,2556,...|[-209.67369997187...|[1.0,7.5953362756...|       0.0|\n",
            "|  ham|1) Go to write ms...|   141|[1), go, to, writ...|  0.0|  [1), go, write, m...|(10824,[3,4,6,7,1...|(10824,[3,4,6,7,1...|(10825,[3,4,6,7,1...|[-1170.6101616895...|[1.0,1.0597369524...|       0.0|\n",
            "|  ham|   10 min later k...|    17|[10, min, later, ...|  0.0|  [10, min, later, ...|(10824,[65,468,55...|(10824,[65,468,55...|(10825,[65,468,55...|[-221.23849799919...|[0.99999999917411...|       0.0|\n",
            "|  ham|2 and half years ...|    44|[2, and, half, ye...|  0.0|  [2, half, years, ...|(10824,[2,210,346...|(10824,[2,210,346...|(10825,[2,210,346...|[-181.11116310614...|[0.99910999351879...|       0.0|\n",
            "|  ham|2 celebrate my bÂ’...|    29|[2, celebrate, my...|  0.0|  [2, celebrate, bÂ’...|(10824,[2,173,267...|(10824,[2,173,267...|(10825,[2,173,267...|[-219.79779220313...|[1.0,3.8052734712...|       0.0|\n",
            "|  ham|2 laptop... I noe...|    58|[2, laptop..., i,...|  0.0|  [2, laptop..., no...|(10824,[2,24,157,...|(10824,[2,24,157,...|(10825,[2,24,157,...|[-338.71879062766...|[1.0,8.1237123081...|       0.0|\n",
            "+-----+--------------------+------+--------------------+-----+----------------------+--------------------+--------------------+--------------------+--------------------+--------------------+----------+\n",
            "only showing top 20 rows\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "i8MLLg0rSBCM"
      },
      "source": [
        "### Print the schema of the prediction dataframe"
      ],
      "id": "i8MLLg0rSBCM"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0-J0GZPOSBCN",
        "outputId": "00ae75dc-296d-4636-f150-3a2b54b2705f"
      },
      "source": [
        "prediction.printSchema()"
      ],
      "id": "0-J0GZPOSBCN",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "root\n",
            " |-- class: string (nullable = true)\n",
            " |-- text: string (nullable = true)\n",
            " |-- length: integer (nullable = true)\n",
            " |-- tokenizer_text: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- label: double (nullable = false)\n",
            " |-- text_without_stopWords: array (nullable = true)\n",
            " |    |-- element: string (containsNull = true)\n",
            " |-- count_vec: vector (nullable = true)\n",
            " |-- tf_idf: vector (nullable = true)\n",
            " |-- features: vector (nullable = true)\n",
            " |-- rawPrediction: vector (nullable = true)\n",
            " |-- probability: vector (nullable = true)\n",
            " |-- prediction: double (nullable = false)\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sMXNb0_USBCN"
      },
      "source": [
        "## Model Evaluation\n",
        "- Use <b>MulticlassClassificationEvaluator</b> to calculate the <b>f1_score</b>."
      ],
      "id": "sMXNb0_USBCN"
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TYZ_0b4FSBCN"
      },
      "source": [
        "from pyspark.ml.evaluation import MulticlassClassificationEvaluator"
      ],
      "id": "TYZ_0b4FSBCN",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HSO-ZWRVSBCO",
        "outputId": "b73a0b4d-492e-407e-88dd-dd7402ccd1b0"
      },
      "source": [
        "evaluator = MulticlassClassificationEvaluator(metricName='f1')\n",
        "f1_score = evaluator.evaluate(prediction)\n",
        "\n",
        "print(\"f1_score is: {}\".format(f1_score))"
      ],
      "id": "HSO-ZWRVSBCO",
      "execution_count": null,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1_score is: 0.9761584458866029\n"
          ]
        }
      ]
    }
  ]
}